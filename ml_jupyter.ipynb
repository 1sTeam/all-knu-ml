{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a298f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model용 import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dataset as dt\n",
    "import crawling as cl\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM,Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflowjs as tfjs\n",
    "from pandas.plotting import scatter_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e86b4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset용 improt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pykospacing import Spacing\n",
    "from konlpy.tag import Komoran\n",
    "import urllib.request\n",
    "from soynlp import DoublespaceLineCorpus\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.normalizer import *\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b164e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text 전처리 class\n",
    "class text_processing():\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "        self.token_size = 0\n",
    "\n",
    "    def text_normalization(self):\n",
    "        self.dataset = self.dataset.dropna(how = 'any')\n",
    "        self.dataset['Title'] = self.dataset['Title'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z ]\",\"\")\n",
    "        self.dataset['Title'].replace('', np.nan, inplace=True)\n",
    "        return self.dataset\n",
    "\n",
    "    def text_tokenization(self):\n",
    "        komoran = Komoran(userdic= 'dic.txt')\n",
    "        temp = []\n",
    "        for sentence in self.dataset['Title']:\n",
    "            temp.append(komoran.nouns(sentence))\n",
    "        self.dataset['Title'] = temp\n",
    "        return self.dataset\n",
    "\n",
    "    def text_integer(self):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(self.dataset['Title'])\n",
    "        token_size = len(tokenizer.word_index)\n",
    "\n",
    "        tokenizer = Tokenizer(token_size)\n",
    "        tokenizer.fit_on_texts(self.dataset['Title'])\n",
    "\n",
    "        self.dataset['Title'] = tokenizer.texts_to_sequences(self.dataset['Title'])\n",
    "        self.token_size = token_size\n",
    "        return self.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a04982d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predict(dataset):\n",
    "    dataset = pd.read_csv('dataset.csv', low_memory=False)\n",
    "    y_train = np.array(dataset['Binary'])\n",
    "    x_train = []\n",
    "    for index in dataset['Title']:\n",
    "        x_train.append(np.fromstring(index, dtype=int, sep=','))\n",
    "\n",
    "    x_train = pad_sequences(x_train, maxlen = 12)\n",
    "\n",
    "    # x_train, x_test, y_train, y_test = train_test_split(\n",
    "    #     x_train, y_train, test_size=0.2, random_state=1)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # model.add(Embedding(1152, 100))\n",
    "    # model.add(LSTM(128))\n",
    "    # model.add(Dense(1, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # model.add(layers.Dense(16, activation='relu', input_shape=(len(dataset),)))\n",
    "    # model.add(layers.Dense(16, activation='relu'))\n",
    "    # model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "    # history = model.fit(x_train, y_train, epochs=15, callbacks=[es, mc], batch_size=30, validation_split=0.2)\n",
    "    history = model.fit(x_train, y_train, epochs=30, batch_size=60, validation_split=0.2)\n",
    "\n",
    "    # loaded_model = load_model('best_model.h5')\n",
    "    return model\n",
    "\n",
    "\n",
    "def sentiment_text_processing():\n",
    "    m_dataset = cl.single_page_crawling_for_modeling()\n",
    "    print(m_dataset['Title'])\n",
    "    m_dataset = dt.text_processing(m_dataset)\n",
    "    m_dataset.text_normalization()\n",
    "    m_dataset.text_tokenization()\n",
    "    m_dataset.text_integer()\n",
    "\n",
    "    return m_dataset.dataset\n",
    "  \n",
    "def sentiment_predict(m_dataset, model):\n",
    "    x_train = pad_sequences(m_dataset['Title'], maxlen = 12)\n",
    "\n",
    "    # for sentence in x_train:\n",
    "    score = model.predict(x_train)\n",
    "\n",
    "    print(score)\n",
    "\n",
    "    for s in score:\n",
    "        s = float(s)\n",
    "        if(s > 0.5):\n",
    "          print(\"{:.2f}% 확률로 비교과프로그램입니다.\\n\".format(s * 100))\n",
    "        else:\n",
    "          print(\"{:.2f}% 확률로 비교과 프로그램이 아닙니다.\\n\".format((1 - s) * 100))\n",
    "\n",
    "def visualize(m_dataset,m_target):\n",
    "    x_train = []\n",
    "    for index in dataset['Title']:\n",
    "        x_train.append(np.fromstring(index, dtype=int, sep=','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61945b2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'KafkaProducer' from partially initialized module 'kafka' (most likely due to a circular import) (C:\\Users\\wkdgn\\Desktop\\Study\\3-2\\SelfDirectedProjectII\\all-knu-ml\\kafka.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-35a601cd1123>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkafka\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKafkaProducer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjson\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdumps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mproducer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKafkaProducer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gzip'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbootstrap_servers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'localhost:9092'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_serializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Study\\3-2\\SelfDirectedProjectII\\all-knu-ml\\kafka.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkafka\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKafkaProducer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjson\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdumps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mproducer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKafkaProducer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gzip'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbootstrap_servers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'localhost:9092'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_serializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'KafkaProducer' from partially initialized module 'kafka' (most likely due to a circular import) (C:\\Users\\wkdgn\\Desktop\\Study\\3-2\\SelfDirectedProjectII\\all-knu-ml\\kafka.py)"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "from json import dumps\n",
    "import time\n",
    "\n",
    "producer = KafkaProducer(acks=0, compression_type='gzip', bootstrap_servers=['localhost:9092'], value_serializer=lambda x: dumps(x).encode('utf-8')) \n",
    "start = time.time() \n",
    "for i in range(10000): \n",
    "    data = {'str' : 'result'+str(i)} \n",
    "    producer.send('quickstart-events', value=data) \n",
    "    producer.flush()\n",
    "print(\"elapsed :\", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd8adb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
